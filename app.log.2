2024-03-13 19:39:57,254 - app - INFO - Received model: claude-3-opus-20240229
2024-03-13 19:39:57,255 - app - INFO - Received temperature: 1
2024-03-13 19:39:57,257 - app - INFO - Using existing conversation with id 310.
2024-03-13 19:40:30,348 - app - INFO - Response from model: It seems that the system message is not being passed to the Anthropic API correctly. Let's make the necessary changes to ensure the system message is included in the API request.

Here's the updated `get_response_from_model` function with the changes:

```python
def get_response_from_model(model, messages, temperature):
    """
    Routes the request to the appropriate API based on the model selected.
    """
    if model in ["gpt-3.5-turbo-0613", "gpt-4-0613", "gpt-4-1106-preview", "gpt-4-0125-preview"]:
        # OpenAI models
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature
        }
        response = openai.ChatCompletion.create(**payload)
        chat_output = response['choices'][0]['message']['content']
    elif model == "claude-3-opus-20240229":
        # Anthropic model
        client = anthropic.Client(api_key=os.environ["ANTHROPIC_API_KEY"])

        # Construct the conversation history for Messages API
        anthropic_messages = []
        system_message = None
        for message in messages:
            if message['role'] == 'system':
                system_message = message['content']
            elif message['role'] == 'user':
                anthropic_messages.append({"role": "user", "content": message['content']})
            elif message['role'] == 'assistant':
                anthropic_messages.append({"role": "assistant", "content": message['content']})

        # Send the conversation to the Anthropic Messages API
        response = client.messages.create(
            model=model,
            messages=anthropic_messages,
            max_tokens=1000,  # Specify the maximum number of tokens to generate
            temperature=temperature,
            system_prompt=system_message  # Include the system message in the API request
        )
        chat_output = response.content[0].text  # Extract the text content from the first ContentBlock
    else:
        chat_output = "Sorry, the selected model is not supported yet."
   
    return chat_output
```

The main change is in the Anthropic API request. We include the `system_prompt` parameter and pass the `system_message` variable to it. This ensures that the system message is sent to the Anthropic API along with the conversation history.

```python
response = client.messages.create(
    model=model,
    messages=anthropic_messages,
    max_tokens=1000,  # Specify the maximum number of tokens to generate
    temperature=temperature,
    system_prompt=system_message  # Include the system message in the API request
)
```

With this modification, the system message should now be correctly passed to the Anthropic API, and the AI assistant should respond accordingly.

Make sure to update the `app.py` file with this modified `get_response_from_model` function.

Let me know if you have any further questions or if there's anything else I can assist you with!
